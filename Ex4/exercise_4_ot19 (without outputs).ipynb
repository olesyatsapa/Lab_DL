{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "#import tempfile\n",
    "\n",
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "#import numpy as np\n",
    "import random\n",
    "\n",
    "class Data:\n",
    "  def __init__(self):\n",
    "    with h5py.File(\"cell_data.h5\", \"r\") as data:\n",
    "      self.train_images = [data[\"/train_image_{}\".format(i)][:] for i in range(28)]\n",
    "      self.train_labels = [data[\"/train_label_{}\".format(i)][:] for i in range(28)]\n",
    "      self.test_images = [data[\"/test_image_{}\".format(i)][:] for i in range(3)]\n",
    "      self.test_labels = [data[\"/test_label_{}\".format(i)][:] for i in range(3)]\n",
    "    \n",
    "    self.input_resolution = 300\n",
    "    self.label_resolution = 116\n",
    "\n",
    "    self.offset = (300 - 116) // 2\n",
    "\n",
    "  def get_train_image_list_and_label_list(self):\n",
    "    n = random.randint(0, len(self.train_images) - 1)\n",
    "    x = random.randint(0, (self.train_images[n].shape)[1] - self.input_resolution - 1)\n",
    "    y = random.randint(0, (self.train_images[n].shape)[0] - self.input_resolution - 1)\n",
    "    image = self.train_images[n][y:y + self.input_resolution, x:x + self.input_resolution, :]\n",
    "\n",
    "    x += self.offset\n",
    "    y += self.offset\n",
    "    label = self.train_labels[n][y:y + self.label_resolution, x:x + self.label_resolution]\n",
    "    \n",
    "    return [image], [label]\n",
    "\n",
    "  def get_test_image_list_and_label_list(self):\n",
    "    coord_list = [[0,0], [0, 116], [0, 232], \n",
    "                  [116,0], [116, 116], [116, 232],\n",
    "                  [219,0], [219, 116], [219, 232]]\n",
    "    \n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for image_id in range(3):\n",
    "      for y, x in coord_list:\n",
    "        image = self.test_images[image_id][y:y + self.input_resolution, x:x + self.input_resolution, :]\n",
    "        image_list.append(image)\n",
    "        x += self.offset\n",
    "        y += self.offset\n",
    "        label = self.test_labels[image_id][y:y + self.label_resolution, x:x + self.label_resolution]\n",
    "        label_list.append(label)\n",
    "    \n",
    "\n",
    "    return image_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Data()\n",
    "print(len(d.train_images))\n",
    "print(d.train_images[0].shape)\n",
    "im, lab = d.get_train_image_list_and_label_list()\n",
    "print(im[0].shape)\n",
    "print(len(lab))\n",
    "print(lab[0].shape)\n",
    "print(lab[0])\n",
    "plt.imshow(lab[0], cmap='gray')\n",
    "plt.show()\n",
    "tmp=np.reshape(im[0],[300,300])\n",
    "plt.imshow(tmp, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "im, lab = d.get_test_image_list_and_label_list()\n",
    "\n",
    "print(len(lab))\n",
    "print(lab[0].shape)\n",
    "print(lab[0])\n",
    "plt.imshow(lab[0], cmap='gray')\n",
    "plt.show()\n",
    "tmp=np.reshape(im[0],[300,300])\n",
    "plt.imshow(tmp, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "    #v = tf.get_variable(\"v\", shape=shape, initializer=tf.glorot uniform initializer())\n",
    "    #return v\n",
    "    \n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "def print_size(l):\n",
    "    print(str(l)+' size=',l.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def u_net(x):\n",
    "    mini_batch_size = 1\n",
    "    # Reshape the image\n",
    "    print_size(x)\n",
    "    \n",
    "    x_image = tf.reshape(x, [mini_batch_size, 300, 300, 1])\n",
    "    print_size(x_image)\n",
    "    \n",
    "    # convolution 1. Output 298*298*32\n",
    "    W_conv1 = weight_variable([3, 3, 1, 32])\n",
    "    b_conv1 = bias_variable([32])    \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    print_size(h_conv1)\n",
    "    \n",
    "    # convolution 2. Output 296*296*32\n",
    "    W_conv2 = weight_variable([3, 3, 32, 32])\n",
    "    b_conv2 = bias_variable([32])    \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "    print_size(h_conv2)\n",
    "    \n",
    "    # max pool 3. Output 148*148*32\n",
    "    h_pool3 = max_pool_2x2(h_conv2)\n",
    "    print_size(h_pool3)\n",
    "    \n",
    "    # convolution 4. Output 146*146*64\n",
    "    W_conv4 = weight_variable([3, 3, 32, 64])\n",
    "    b_conv4 = bias_variable([64])     \n",
    "    h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "    print_size(h_conv4)\n",
    "    \n",
    "    # convolution 5. Output 144*144*64\n",
    "    W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "    b_conv5 = bias_variable([64])     \n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5) + b_conv5)\n",
    "    print_size(h_conv5)\n",
    "    \n",
    "    # max pool 6. Output 72*72*64\n",
    "    h_pool6 = max_pool_2x2(h_conv5)\n",
    "    print_size(h_pool6)\n",
    "    \n",
    "    # convolution 7. Output 70*70*128\n",
    "    W_conv7 = weight_variable([3, 3, 64, 128])\n",
    "    b_conv7 = bias_variable([128])     \n",
    "    h_conv7 = tf.nn.relu(conv2d(h_pool6, W_conv7) + b_conv7)\n",
    "    print_size(h_conv7)\n",
    "    \n",
    "    # convolution 8. Output 68*68*128\n",
    "    W_conv8 = weight_variable([3, 3, 128, 128])\n",
    "    b_conv8 = bias_variable([128])     \n",
    "    h_conv8 = tf.nn.relu(conv2d(h_conv7, W_conv8) + b_conv8)\n",
    "    print_size(h_conv8)\n",
    "    \n",
    "    # max pool 9. Output 34*34*128\n",
    "    h_pool9 = max_pool_2x2(h_conv8)\n",
    "    print_size(h_pool9)\n",
    "    \n",
    "    # convolution 10. Output 32*32*256\n",
    "    W_conv10 = weight_variable([3, 3, 128, 256])\n",
    "    b_conv10 = bias_variable([256])     \n",
    "    h_conv10 = tf.nn.relu(conv2d(h_pool9, W_conv10) + b_conv10)\n",
    "    print_size(h_conv10)\n",
    "    \n",
    "    # convolution 11. Output 30*30*256\n",
    "    W_conv11 = weight_variable([3, 3, 256, 256])\n",
    "    b_conv11 = bias_variable([256])     \n",
    "    h_conv11 = tf.nn.relu(conv2d(h_conv10, W_conv11) + b_conv11)\n",
    "    print_size(h_conv11)\n",
    "    \n",
    "    # max pool 12. Output 15*15*256\n",
    "    h_pool12 = max_pool_2x2(h_conv11)\n",
    "    print_size(h_pool12)\n",
    "    \n",
    "    # convolution 13. Output 13*13*512\n",
    "    W_conv13 = weight_variable([3, 3, 256, 512])\n",
    "    b_conv13 = bias_variable([512])     \n",
    "    h_conv13 = tf.nn.relu(conv2d(h_pool12, W_conv13) + b_conv13)\n",
    "    print_size(h_conv13)\n",
    "    \n",
    "    # convolution 14. Output 11*11*512\n",
    "    W_conv14 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv14 = bias_variable([512])     \n",
    "    h_conv14 = tf.nn.relu(conv2d(h_conv13, W_conv14) + b_conv14)\n",
    "    print_size(h_conv14)\n",
    "    \n",
    "    # up-convolition 15. Output 22*22*256\n",
    "    W_uconv15 = weight_variable([2, 2, 256, 512])   \n",
    "    stride  = [1, 2, 2, 1]\n",
    "    h_uconv15 = tf.nn.conv2d_transpose(h_conv14, W_uconv15,[mini_batch_size, 22, 22, 256],stride, padding='VALID')\n",
    "    print_size(h_uconv15)\n",
    "    # concat with croped copy from layer 11. Output 22*22*512\n",
    "    h_conv11_crop = tf.image.resize_image_with_crop_or_pad(h_conv11, 22, 22)\n",
    "    print_size(h_conv11_crop)\n",
    "    #print(h_conv11_crop.get_shape().as_list())\n",
    "    h_uconv15_ = tf.concat([h_conv11_crop, h_uconv15], 3)\n",
    "    print_size(h_uconv15_)\n",
    "    \n",
    "    # convolution 16. Output 20*20*256\n",
    "    W_conv16 = weight_variable([3, 3, 512, 256])\n",
    "    b_conv16 = bias_variable([256])     \n",
    "    h_conv16 = tf.nn.relu(conv2d(h_uconv15_, W_conv16) + b_conv16)\n",
    "    print_size(h_conv16)\n",
    "    \n",
    "    # convolution 17. Output 18*18*256\n",
    "    W_conv17 = weight_variable([3, 3, 256, 256])\n",
    "    b_conv17 = bias_variable([256])     \n",
    "    h_conv17 = tf.nn.relu(conv2d(h_conv16, W_conv17) + b_conv17)\n",
    "    print_size(h_conv17)\n",
    "    \n",
    "    # up-convolition 18. Output 36*36*128\n",
    "    W_uconv18 = weight_variable([2, 2, 128, 256])   \n",
    "    stride  = [1, 2, 2, 1]\n",
    "    h_uconv18 = tf.nn.conv2d_transpose(h_conv17, W_uconv18,[mini_batch_size, 36, 36, 128],stride, padding='VALID') \n",
    "    print_size(h_uconv18)\n",
    "    # concat with croped copy from layer 8. Output 36*36*256\n",
    "    h_conv8_crop = tf.image.resize_image_with_crop_or_pad(h_conv8, 36, 36)\n",
    "    print_size(h_conv8_crop)\n",
    "    h_uconv18_ = tf.concat([h_conv8_crop, h_uconv18], 3)\n",
    "    print_size(h_uconv18_)\n",
    "    \n",
    "    # convolution 19. Output 34*34*128\n",
    "    W_conv19 = weight_variable([3, 3, 256, 128])\n",
    "    b_conv19 = bias_variable([128])     \n",
    "    h_conv19 = tf.nn.relu(conv2d(h_uconv18_, W_conv19) + b_conv19)\n",
    "    print_size(h_conv19)\n",
    "    \n",
    "    # convolution 20. Output 32*32*128\n",
    "    W_conv20 = weight_variable([3, 3, 128, 128])\n",
    "    b_conv20 = bias_variable([128])     \n",
    "    h_conv20 = tf.nn.relu(conv2d(h_conv19, W_conv20) + b_conv20)\n",
    "    print_size(h_conv20)\n",
    "    \n",
    "    \n",
    "    # up-convolition 21. Output 64*64*64\n",
    "    W_uconv21 = weight_variable([2, 2, 64, 128])   \n",
    "    stride  = [1, 2, 2, 1]\n",
    "    h_uconv21 = tf.nn.conv2d_transpose(h_conv20, W_uconv21,[mini_batch_size, 64, 64, 64],stride, padding='VALID') \n",
    "    print_size(h_uconv21)\n",
    "    # concat with croped copy from layer 5. Output 64*64*128\n",
    "    h_conv5_crop = tf.image.resize_image_with_crop_or_pad(h_conv5, 64, 64)\n",
    "    print_size(h_conv5_crop)\n",
    "    h_uconv21_ = tf.concat([h_conv5_crop, h_uconv21], 3)\n",
    "    print_size(h_uconv21_)\n",
    "    \n",
    "    # convolution 22. Output 62*62*64\n",
    "    W_conv22 = weight_variable([3, 3, 128, 64])\n",
    "    b_conv22 = bias_variable([64])     \n",
    "    h_conv22 = tf.nn.relu(conv2d(h_uconv21_, W_conv22) + b_conv22)\n",
    "    print_size(h_conv22)\n",
    "    \n",
    "    # convolution 23. Output 60*60*64\n",
    "    W_conv23 = weight_variable([3, 3, 64, 64])\n",
    "    b_conv23 = bias_variable([64])     \n",
    "    h_conv23 = tf.nn.relu(conv2d(h_conv22, W_conv23) + b_conv23)\n",
    "    print_size(h_conv23)\n",
    "    \n",
    "    # up-convolition 24. Output 120*120*32\n",
    "    W_uconv24 = weight_variable([2, 2, 32, 64])   \n",
    "    stride  = [1, 2, 2, 1]\n",
    "    h_uconv24 = tf.nn.conv2d_transpose(h_conv23, W_uconv24,[mini_batch_size, 120, 120, 32],stride, padding='VALID')\n",
    "    print_size(h_uconv24)\n",
    "    # concat with croped copy from layer 2. Output 120*120*64\n",
    "    h_conv2_crop = tf.image.resize_image_with_crop_or_pad(h_conv2, 120, 120)\n",
    "    print_size(h_conv2_crop)\n",
    "    h_uconv24_ = tf.concat([h_conv2_crop, h_uconv24], 3)\n",
    "    print_size(h_uconv24_)\n",
    "    \n",
    "    # convolution 25. Output 118*118*32\n",
    "    W_conv25 = weight_variable([3, 3, 64, 32])\n",
    "    b_conv25 = bias_variable([32])     \n",
    "    h_conv25 = tf.nn.relu(conv2d(h_uconv24_, W_conv25) + b_conv25)\n",
    "    print_size(h_conv25)\n",
    "    \n",
    "    # convolution 26. Output 116*116*32\n",
    "    W_conv26 = weight_variable([3, 3, 32, 32])\n",
    "    b_conv26 = bias_variable([32])     \n",
    "    h_conv26 = tf.nn.relu(conv2d(h_conv25, W_conv26) + b_conv26)\n",
    "    print_size(h_conv26)\n",
    "    \n",
    "    # convolution 27. Output 116*116*2\n",
    "    W_conv27 = weight_variable([1, 1, 32, 2])\n",
    "    b_conv27 = bias_variable([2])     \n",
    "    h_conv27 = conv2d(h_conv26, W_conv27) + b_conv27\n",
    "    print_size(h_conv27)\n",
    "    \n",
    "    \n",
    "    return h_conv27\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_tr_array=[]\n",
    "acc_te_array=[]    \n",
    "loss_tr_array=[]\n",
    "loss_te_array=[]\n",
    "\n",
    "mini_batch_size=1\n",
    "learning_rate=0.0001\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 300,300,1])\n",
    "y = tf.placeholder(tf.int32, [None, 116,116])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_hat = u_net(x)\n",
    "\n",
    "y_resh = tf.reshape(y, [mini_batch_size*116*116])\n",
    "y_hot=tf.one_hot(y_resh,2)\n",
    "\n",
    "print('y_hot', y_hot.get_shape().as_list())\n",
    "\n",
    "\n",
    "y_hat_resh = tf.reshape(y_hat, [mini_batch_size*116*116, 2])\n",
    "\n",
    "print('y_hat_resh',y_hat_resh.get_shape().as_list())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_hot, logits=y_hat_resh)\n",
    "\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate, 0.95, 0.99).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_hat_resh, 1), tf.argmax(y_hot, 1))\n",
    "correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "device_count = {'GPU': 0})\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "\n",
    "with sess as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    t0 = time.time()\n",
    "    for i in range(40001):\n",
    "        batch = d.get_train_image_list_and_label_list()\n",
    "        \n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1]})\n",
    "       \n",
    "   \n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            prediction = correct_prediction.eval(feed_dict={x: batch[0], y: batch[1]})\n",
    "            #print('prediction.shape',prediction.shape)\n",
    "            train_accuracy = (np.sum(prediction))/(2*prediction.shape[0]-np.sum(prediction))\n",
    "\n",
    "            loss_tr = loss.eval(feed_dict = {x: batch[0], y: batch[1]})\n",
    "\n",
    "            loss_tr_array.append(loss_tr)\n",
    "            acc_tr_array.append(train_accuracy)\n",
    "            \n",
    "            print(\"Batch: \" + str(i) + \" Train Loss value:\" + str(loss_tr)+\" Train Accuracy \"+str(train_accuracy))\n",
    "            \n",
    "            \n",
    "            #test\n",
    "            im, lbl = d.get_test_image_list_and_label_list()\n",
    "            test_accuracy=0.0\n",
    "            for j in range(len(lbl)):\n",
    "                output= y_hat.eval(feed_dict={x: np.reshape(im[j],[1,300,300,1]), y: np.reshape(lbl[j],[1,116,116])})\n",
    "                #print(output.shape)\n",
    "                output=np.argmax(output, axis=-1)\n",
    "                #print(output.shape)\n",
    "                \n",
    "                #prediction\n",
    "                print('Prediction ', j)\n",
    "                pixels_in = np.reshape(output,[116, 116])\n",
    "                plt.imshow(pixels_in, cmap='gray')\n",
    "                plt.show()\n",
    "                #label\n",
    "                print('Label ', j)\n",
    "                pixels_in = np.reshape(lbl[j],[116, 116])\n",
    "                plt.imshow(pixels_in, cmap='gray')\n",
    "                plt.show()\n",
    "                \n",
    "                prediction_test = correct_prediction.eval(feed_dict={x: np.reshape(im[j],[1,300,300,1]), y: np.reshape(lbl[j],[1,116,116])})\n",
    "                test_accuracy = test_accuracy+(np.sum(prediction_test))/(2*prediction_test.shape[0]-np.sum(prediction_test))\n",
    "            \n",
    "            test_accuracy = test_accuracy/len(lbl)\n",
    "            acc_te_array.append(test_accuracy)\n",
    "            print('test_accuracy',test_accuracy)\n",
    "            \n",
    "\n",
    "    t1 = time.time()    \n",
    "    print('Duration: {:.1f}s'.format(t1-t0))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('acc_tr_array', acc_tr_array)  \n",
    "print('acc_te_array', acc_te_array) \n",
    "f_tr = open(\"train.txt\",\"w\") \n",
    "f_te = open(\"test.txt\",\"w\")\n",
    "for t in acc_tr_array:\n",
    "    f_tr.write(str(t)+'\\n') \n",
    "for t in acc_te_array:\n",
    "    f_te.write(str(t)+'\\n') \n",
    "#f_te.write(acc_te_array) \n",
    "\n",
    "\n",
    "f_tr.close()\n",
    "f_te.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(valid_acc_arr)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "x=[]\n",
    "for i in range(len(acc_te_array)):\n",
    "    x.append(i*500)\n",
    "print(len(x))\n",
    "#print(acc_tr_array)\n",
    "print(len(acc_te_array))      \n",
    "plt.plot(x, -np.log(acc_tr_array),label='train acc', linewidth=2.0)\n",
    "plt.plot(x, -np.log(acc_te_array), label='test acc', linewidth=2.0)\n",
    "#plt.plot(np.log(loss_tr_array1), label='Log of train loss. Learning rate 1e-1', linewidth=2.0)\n",
    "plt.legend(loc='best') \n",
    "plt.ylabel('Negative log of accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
